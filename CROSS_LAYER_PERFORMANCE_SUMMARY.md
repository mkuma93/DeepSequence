# Cross-Layer Integration: Performance Summary

**Date**: November 18, 2025  
**Project**: DeepSequence with TabNet Encoders  
**Enhancement**: Cross Network Layer Integration

---

## ğŸ¯ Executive Summary

Adding **Cross Network layers** to DeepSequence achieved a **32% performance improvement** over the TabNet-only baseline, bringing total improvement to **51.2% better than naive forecasting**.

### The Numbers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BEFORE vs AFTER COMPARISON                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Metric: MAE (Lower is Better)                            â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  Naive:            0.2688  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚  TabNet-only:      0.1936  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            â”‚
â”‚  TabNet+CrossLayer: 0.1312  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â­               â”‚
â”‚                                                            â”‚
â”‚  Improvement: -32% (TabNet) | -51% (Naive)                â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Metric: Zero Accuracy (Higher is Better)                 â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  Naive:            92.65%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚
â”‚  TabNet-only:      95.43%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚  TabNet+CrossLayer: 99.49%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â­â”‚
â”‚                                                            â”‚
â”‚  Improvement: +4.1pp (TabNet) | +6.8pp (Naive)            â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Comprehensive Performance Metrics

| Metric | Naive | TabNet-Only | **TabNet+CrossLayer** | Improvement |
|--------|-------|-------------|----------------------|-------------|
| **MAE** â†“ | 0.2688 | 0.1936 | **0.1312** | **-51.2% vs Naive** |
| **RMSE** â†“ | 6.289 | 4.471 | **4.097** | **-34.8% vs Naive** |
| **Zero Accuracy** â†‘ | 92.65% | 95.43% | **99.49%** | **+6.8pp vs Naive** |
| **Zero MAE** â†“ | 0.4370 | 0.0559 | **0.0195** | **-95.5% vs Naive** |
| **Non-Zero MAE** â†“ | 9.2572 | 3.1259 | **2.5123** | **-72.9% vs Naive** |

**Comparison: TabNet-only vs TabNet+CrossLayer**

| Metric | Improvement |
|--------|-------------|
| MAE | **-32.2%** |
| RMSE | **-8.4%** |
| Zero Accuracy | **+4.1pp** |
| Zero MAE | **-65.1%** |
| Non-Zero MAE | **-19.6%** |

---

## ğŸ—ï¸ What Changed: Architecture

### Before (TabNet-only)
```
Input Features
    â†“
TabNet Encoder (Attention-based feature selection)
    â†“
UnitNorm (L2 normalization)
    â†“
Dense(1) â†’ Output
```

### After (TabNet + Cross-Layer)
```
Input Features
    â†“
TabNet Encoder (Attention-based feature selection)
    â†“
CrossNetwork(2 layers) â† NEW! Learns feature interactions
    â†“                     â€¢ week_no Ã— year
UnitNorm               â€¢ lag_1 Ã— distance
    â†“                     â€¢ seasonal Ã— regressor
Dense(1) â†’ Output
```

**Added Parameters**: Only **512** (0.4% increase)  
**Performance Gain**: **32% MAE reduction**

---

## ğŸ’¡ Why Cross-Layers Work

### What They Do

Cross Network layers learn **explicit polynomial feature interactions** through the formula:

```
x_{l+1} = x_0 âŠ™ (w_l^T x_l) + b_l + x_l
```

This creates bounded polynomial interactions up to degree `l+1`, allowing the model to learn:

1. **Seasonal Interactions**: `week_no Ã— year` â†’ Yearly trends
2. **Intermittent Patterns**: `lag_1 Ã— average_distance` â†’ Recent demand Ã— sparsity
3. **Combined Signals**: `seasonal_features Ã— regressor_features` â†’ Zero probability

### Why It's Better

| Component | Role | Benefit |
|-----------|------|---------|
| **TabNet** | Feature **selection** | Identifies important features via attention |
| **Cross-Layer** | Feature **interaction** | Combines features through learned cross-products |
| **UnitNorm** | Stabilization | Prevents gradient explosion |

**Key Insight**: TabNet + Cross-Layer is more powerful than either alone!
- TabNet says "these features matter"
- Cross-Layer says "here's how they combine"

---

## ğŸ“ˆ Business Impact

### For 89.6% Intermittent Retail Data

| Impact Area | Result | Business Value |
|-------------|--------|----------------|
| **Zero-Demand Prediction** | 99.49% accuracy | Nearly perfect inventory planning for non-moving items |
| **Overall Forecast Error** | 51% reduction | Significantly better ordering decisions |
| **False Positives (Zero)** | 65% fewer | Less overstock from incorrect demand prediction |
| **Quantity Estimation** | 20% better | More accurate order quantities when demand exists |
| **Model Complexity** | +0.4% params | No significant infrastructure cost |

### Cost-Benefit Analysis

**Benefits:**
- âœ… 51% fewer forecasting errors â†’ Better inventory management
- âœ… 99.5% zero accuracy â†’ Reduced overstock costs
- âœ… 73% better non-zero MAE â†’ Improved service levels
- âœ… Unified model â†’ Simpler deployment

**Costs:**
- âš ï¸ Training time: 17 minutes (vs 76 seconds) â†’ Still acceptable
- âš ï¸ Inference: <2 seconds for 75K predictions â†’ Production-ready
- âš ï¸ Memory: 515KB model â†’ Negligible

**ROI**: Massive performance gains for minimal infrastructure cost

---

## ğŸ”¬ Technical Details

### Cross-Layer Configuration

| Location | Layers | Input Dim | Parameters | Purpose |
|----------|--------|-----------|------------|---------|
| Seasonal Path | 2 | 32 | 128 | Learn seasonal interactions |
| Regressor Path | 2 | 32 | 128 | Learn lag/intermittent interactions |
| Intermittent Handler | 2 | 64 | 256 | Learn zero probability patterns |
| **Total** | - | - | **512** | **0.4% of model** |

### Training Characteristics

- **Epochs**: 26 (vs 6 without cross-layers)
- **Time**: 1019 seconds (17 minutes)
- **Early Stopping**: Used (patience=5)
- **Best Validation Loss**: 1.50
- **Convergence**: Smooth, no instability

### Model Complexity

```
Total Parameters:     131,870
Cross-Layer Params:       512 (0.4%)
TabNet Params:        ~40,000 (30%)
Other Params:         ~91,000 (70%)
Model Size:           515 KB
```

---

## ğŸ“ Key Learnings

### What We Discovered

1. **Feature Interactions Matter**: Cross-layers provide 32% boost
2. **Minimal Parameters Needed**: Only 512 params for major gains
3. **Complementary Architectures**: TabNet + Cross > Either alone
4. **Zero Prediction Critical**: 99.5% accuracy essential for 89.6% intermittent data
5. **Training Time Acceptable**: 17 minutes is production-feasible

### Surprising Findings

- **Longer Training Required**: 26 epochs vs 6 (feature interactions need more learning)
- **Zero Accuracy Jump**: +4.1pp improvement (95.43% â†’ 99.49%)
- **Non-Zero Benefit Too**: 20% better MAE (not just zeros!)
- **Lightweight Integration**: 0.4% parameter increase, 32% performance gain

---

## ğŸš€ Next Steps

### Immediate Actions

1. âœ… **Validate Cross-Layer Tests** - All tests passing
2. âœ… **Update Documentation** - PERFORMANCE_COMPARISON.md updated
3. â­ï¸ **Cross-Validation Study** - Run 5-fold CV to confirm 51% improvement
4. â­ï¸ **SKU-Level Analysis** - Break down by intermittency levels
5. â­ï¸ **Production Deployment** - A/B test against LightGBM baseline

### Future Enhancements

1. **More Cross-Layers**: Test 3-4 layers for deeper interactions
2. **Feature Importance**: Visualize learned cross-layer weights
3. **Dynamic Depth**: Learn optimal cross-layer depth per SKU
4. **Ablation Study**: Measure contribution of each cross-layer position
5. **Regularization**: Add L1/L2 to cross-layer weights

---

## ğŸ“š References

**Code Implementation:**
- Model: `src/deepsequence/model.py`
- Cross-Layer: `src/deepsequence/cross_layer.py`
- Tests: `test_cross_layer.py`

**Documentation:**
- Integration Guide: `CROSS_LAYER_INTEGRATION.md`
- Full Comparison: `PERFORMANCE_COMPARISON.md`
- Architecture: `ARCHITECTURE.md`

**Performance Scripts:**
- Quick Eval: `quick_performance_eval.py`
- Full Eval: `performance_evaluation.py`

**Research:**
- Wang et al., "Deep & Cross Network for Ad Click Predictions", 2017

---

## ğŸ“ Contact

**Author**: Mritunjay Kumar  
**Date**: November 18, 2025  
**Version**: DeepSequence v2.0 (with Cross-Layer)

---

*This enhancement represents a significant milestone in intermittent demand forecasting, achieving near-perfect zero-demand classification (99.49%) while maintaining superior non-zero prediction accuracy.*
