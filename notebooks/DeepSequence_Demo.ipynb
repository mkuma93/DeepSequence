{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7a4c34",
   "metadata": {},
   "source": [
    "# DeepSequence: SKU-Level Forecasting Demo\n",
    "\n",
    "This notebook demonstrates how to use DeepSequence, a Prophet-inspired deep learning architecture for SKU-level forecasting.\n",
    "\n",
    "## Overview\n",
    "\n",
    "DeepSequence combines:\n",
    "- **Seasonal Components**: Captures weekly, monthly, and yearly seasonality\n",
    "- **Regression Components**: Models trends and exogenous variables\n",
    "- **Deep Learning**: Leverages neural networks for complex pattern recognition\n",
    "\n",
    "**Author**: Mritunjay Kumar  \n",
    "**Year**: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b639249",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee75587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Import DeepSequence components\n",
    "from deepsequence import (SeasonalComponent, RegressorComponent, \n",
    "                       DeepSequenceModel, create_time_features, \n",
    "                       prepare_data, train_val_test_split)\n",
    "from deepsequence.config import DATA_DIR, MODEL_DIR\n",
    "from deepsequence.activations import CUSTOM_ACTIVATIONS\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9222849",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = os.path.join(DATA_DIR, 'stock_week_cluster.csv')\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "# Convert date\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data['ds'].min()} to {data['ds'].max()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd03f21",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68620821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time features\n",
    "data = create_time_features(data, date_col='ds')\n",
    "\n",
    "# Filter data (remove 2009 as it's anomalous)\n",
    "data = data[data['year'] >= 2010].copy()\n",
    "\n",
    "# Prepare data with standardization and lags\n",
    "data = prepare_data(\n",
    "    data,\n",
    "    target_col='Quantity',\n",
    "    id_col='StockCode',\n",
    "    standardize=True,\n",
    "    create_lags=True,\n",
    "    lag_periods=[1, 4, 52]\n",
    ")\n",
    "\n",
    "print(\"✓ Feature engineering complete!\")\n",
    "print(f\"Features: {data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d1b61",
   "metadata": {},
   "source": [
    "## 4. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, val, test = train_val_test_split(\n",
    "    data,\n",
    "    date_col='ds',\n",
    "    val_weeks=8,\n",
    "    test_weeks=0  # No test set for now, using val as test\n",
    ")\n",
    "\n",
    "print(f\"Train: {train.shape} | Val: {val.shape}\")\n",
    "print(f\"Train date range: {train['ds'].min()} to {train['ds'].max()}\")\n",
    "print(f\"Val date range: {val['ds'].min()} to {val['ds'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da79f",
   "metadata": {},
   "source": [
    "## 5. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfuture.utils import encode_categorical\n",
    "\n",
    "# Encode StockCode\n",
    "train, encoder = encode_categorical(train, 'StockCode')\n",
    "val, _ = encode_categorical(val, 'StockCode', encoder=encoder)\n",
    "\n",
    "# Rename for model\n",
    "train['id_cat'] = train['StockCode_encoded']\n",
    "val['id_cat'] = val['StockCode_encoded']\n",
    "\n",
    "print(\"✓ Categorical encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c04acb",
   "metadata": {},
   "source": [
    "## 6. Prepare Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "categorical_features = ['week_no', 'wom', 'cluster']\n",
    "target = ['tQuantity']\n",
    "id_var = 'id_cat'\n",
    "\n",
    "# Prepare time series data\n",
    "train_ts = train[['id_cat', 'ds', 'tQuantity']].copy()\n",
    "val_ts = val[['id_cat', 'ds', 'tQuantity']].copy()\n",
    "\n",
    "# Prepare exogenous variables\n",
    "exog_cols = ['price', 'holiday', 'lag1', 'lag4', 'lag52'] + categorical_features\n",
    "train_exog = train[exog_cols].copy()\n",
    "val_exog = val[exog_cols].copy()\n",
    "\n",
    "# Context variables (continuous)\n",
    "context_vars = [col for col in exog_cols if col not in categorical_features]\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Context variables: {context_vars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24324e",
   "metadata": {},
   "source": [
    "## 7. Build DeepSequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsequence.utils import encode_categorical\n",
    "\n",
    "# Initialize Seasonal Component\n",
    "seasonal = SeasonalComponent(\n",
    "    data=train_ts,\n",
    "    target=target,\n",
    "    id_var=id_var,\n",
    "    horizon=8,\n",
    "    weekly=True,\n",
    "    monthly=True,\n",
    "    yearly=True,\n",
    "    unit='w'\n",
    ")\n",
    "\n",
    "# Create seasonal features\n",
    "seasonal.seasonal_feature()\n",
    "\n",
    "# Build seasonal model\n",
    "seasonal.seasonal_model(\n",
    "    hidden=1,\n",
    "    hidden_unit=4,\n",
    "    hidden_act=CUSTOM_ACTIVATIONS['mish'],\n",
    "    output_act=CUSTOM_ACTIVATIONS['swish'],\n",
    "    reg=0.011,\n",
    "    embed_size=50,\n",
    "    drop_out=0.1\n",
    ")\n",
    "\n",
    "print(\"✓ Seasonal component built!\")\n",
    "seasonal.s_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Regressor Component\n",
    "regressor = RegressorComponent(\n",
    "    ts=train_ts,\n",
    "    exog=train_exog,\n",
    "    target=target,\n",
    "    id_var=id_var,\n",
    "    categorical_var=categorical_features,\n",
    "    context_variable=context_vars,\n",
    "    constraint=None,\n",
    "    embed_size=50,\n",
    "    lat_unit=4,\n",
    "    lattice_size=4,\n",
    "    hidden_unit=4,\n",
    "    hidden_act=CUSTOM_ACTIVATIONS['mish'],\n",
    "    output_act=CUSTOM_ACTIVATIONS['listh'],\n",
    "    hidden_layer=1,\n",
    "    drop_out=0.1,\n",
    "    L1=0.032,\n",
    "    rnge=0.8\n",
    ")\n",
    "\n",
    "# Build regressor model (using seasonal ID input for shared embedding)\n",
    "regressor.reg_model(id_input=seasonal.s_model.input[-1])\n",
    "\n",
    "print(\"✓ Regressor component built!\")\n",
    "regressor.combined_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into DeepSequence\n",
    "model = DeepSequenceModel(mode='additive')\n",
    "model.build(seasonal, regressor)\n",
    "\n",
    "print(\"✓ DeepSequence assembled!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640f9bb",
   "metadata": {},
   "source": [
    "## 8. Prepare Training Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_inputs(seasonal_comp, regressor_comp, ts_data, exog_data, val=False):\n",
    "    \"\"\"\n",
    "    Prepare inputs for DeepSequence model.\n",
    "    \"\"\"\n",
    "    seasonal_inputs = []\n",
    "    regressor_inputs = []\n",
    "    \n",
    "    # Get seasonal features\n",
    "    if val:\n",
    "        sr_df = seasonal_comp.seasonal_feature(ts_data)\n",
    "    else:\n",
    "        sr_df = seasonal_comp.sr_df\n",
    "    \n",
    "    # Seasonal inputs\n",
    "    for col in sr_df.columns:\n",
    "        if col not in ['ds']:\n",
    "            seasonal_inputs.append(sr_df[col].values)\n",
    "    \n",
    "    # Regressor inputs\n",
    "    for input_name in regressor_comp.input_names:\n",
    "        regressor_inputs.append(exog_data[input_name].values)\n",
    "    \n",
    "    return [seasonal_inputs, regressor_inputs]\n",
    "\n",
    "# Prepare training and validation inputs\n",
    "train_inputs = prepare_model_inputs(seasonal, regressor, train_ts, train_exog, val=False)\n",
    "val_inputs = prepare_model_inputs(seasonal, regressor, val_ts, val_exog, val=True)\n",
    "\n",
    "# Get targets\n",
    "train_y = train['tQuantity'].values\n",
    "val_y = val['tQuantity'].values\n",
    "\n",
    "print(f\"✓ Inputs prepared!\")\n",
    "print(f\"Train samples: {len(train_y)}\")\n",
    "print(f\"Val samples: {len(val_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587bd0a",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0face17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mape', learning_rate=0.001)\n",
    "\n",
    "# Train model\n",
    "checkpoint_path = os.path.join(MODEL_DIR, 'deepsequence_best.h5')\n",
    "\n",
    "history = model.fit(\n",
    "    train_input=train_inputs,\n",
    "    train_target=train_y,\n",
    "    val_input=val_inputs,\n",
    "    val_target=val_y,\n",
    "    epochs=500,\n",
    "    batch_size=512,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    patience=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a2810",
   "metadata": {},
   "source": [
    "## 10. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4827d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss (MAPE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='orange')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation MAPE: {min(history.history['val_loss']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f07be4",
   "metadata": {},
   "source": [
    "## 11. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b773bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "val_pred = model.predict(val_inputs)\n",
    "\n",
    "# Add predictions to validation data\n",
    "val['forecast'] = val_pred\n",
    "\n",
    "print(\"✓ Predictions generated!\")\n",
    "val[['StockCode', 'ds', 'tQuantity', 'forecast']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687e97e",
   "metadata": {},
   "source": [
    "## 12. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67580bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsequence.utils import calculate_mape\n",
    "\n",
    "# Calculate MAPE for non-zero values\n",
    "val_nonzero = val[val['tQuantity'] != -1].copy()\n",
    "mape = calculate_mape(val_nonzero['tQuantity'], val_nonzero['forecast'])\n",
    "\n",
    "print(f\"Overall Validation MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Per-SKU performance\n",
    "sku_mape = val_nonzero.groupby('StockCode').apply(\n",
    "    lambda x: calculate_mape(x['tQuantity'], x['forecast'])\n",
    ").reset_index(name='mape')\n",
    "\n",
    "print(f\"\\nTop 10 SKUs by MAPE:\")\n",
    "print(sku_mape.nsmallest(10, 'mape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc786e2",
   "metadata": {},
   "source": [
    "## 13. Visualize Sample Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ceabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few SKUs for visualization\n",
    "sample_skus = val['StockCode'].unique()[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, sku in enumerate(sample_skus):\n",
    "    sku_data = data[data['StockCode'] == sku].sort_values('ds')\n",
    "    sku_val = val[val['StockCode'] == sku].sort_values('ds')\n",
    "    \n",
    "    axes[i].plot(sku_data['ds'], sku_data['tQuantity'], \n",
    "                label='Historical', alpha=0.7)\n",
    "    axes[i].plot(sku_val['ds'], sku_val['forecast'], \n",
    "                label='Forecast', color='red', linewidth=2)\n",
    "    axes[i].set_title(f'SKU: {sku}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d8e49",
   "metadata": {},
   "source": [
    "## 14. Save Model and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f881a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsequence.config import FORECAST_DIR\n",
    "\n",
    "# Save forecasts\n",
    "forecast_path = os.path.join(FORECAST_DIR, 'deepsequence_forecast.csv')\n",
    "val[['StockCode', 'ds', 'Quantity', 'forecast']].to_csv(forecast_path, index=False)\n",
    "\n",
    "print(f\"✓ Forecasts saved to: {forecast_path}\")\n",
    "print(f\"✓ Model saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8f53d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Data loading and preprocessing\n",
    "2. ✅ Feature engineering (time features, lags, standardization)\n",
    "3. ✅ Building DeepSequence architecture\n",
    "4. ✅ Training with early stopping and model checkpointing\n",
    "5. ✅ Evaluation and visualization\n",
    "6. ✅ Generating forecasts\n",
    "\n",
    "### Next Steps\n",
    "- Compare with LightGBM models\n",
    "- Hyperparameter tuning with Optuna\n",
    "- Ensemble multiple models\n",
    "- Deploy for production use"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
